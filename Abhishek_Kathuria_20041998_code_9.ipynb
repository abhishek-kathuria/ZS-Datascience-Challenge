{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "max_features = 50\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data.csv\")\n",
    "df=df.drop([\"game_season\",'team_name','date_of_game','home/away', 'match_id','team_id','match_event_id','lat/lng'], axis=1)\n",
    "df_goal=df['is_goal']\n",
    "df_id=df['shot_id_number']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to string\n",
    "df.area_of_shot=df.area_of_shot.astype(str)\n",
    "df.shot_basics=df.shot_basics.astype(str)\n",
    "df.range_of_shot=df.range_of_shot.astype(str)\n",
    "df.type_of_shot=df.type_of_shot.astype(str)\n",
    "df.type_of_combined_shot=df.type_of_combined_shot.astype(str)\n",
    "df_text=df[[\"area_of_shot\",\"shot_basics\",\"range_of_shot\",\"type_of_shot\",\"type_of_combined_shot\"]].copy()\n",
    "df_text[\"text\"] = df_text[\"area_of_shot\"].map(str) + df_text[\"shot_basics\"]+df_text[\"range_of_shot\"]+df_text[\"type_of_shot\"]+df_text[\"type_of_combined_shot\"]\n",
    "df_text.drop(['area_of_shot','shot_basics', 'range_of_shot', 'type_of_shot','type_of_combined_shot'], axis=1)\n",
    "df=df.drop(['area_of_shot','shot_basics', 'range_of_shot', 'type_of_shot','type_of_combined_shot'], axis=1)\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "df_text[\"text\"] = df_text[\"text\"].apply(lambda x: clean_text(x))\n",
    "# lower\n",
    "df_text[\"text\"] = df_text[\"text\"].apply(lambda x: x.lower())\n",
    "df_text[\"text\"] = df_text[\"text\"].fillna(\"_##_\").values\n",
    "def add_features(df_text):\n",
    "    \n",
    "    df_text['text'] = df_text['text'].apply(lambda x:str(x))\n",
    "    df_text['total_length'] = df_text['text'].apply(len)\n",
    "    df_text['capitals'] = df_text['text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df_text['caps_vs_length'] = df_text.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df_text['num_words'] = df_text.text.str.count('\\S+')\n",
    "    df_text['num_unique_words'] = df_text['text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df_text['words_vs_unique'] = df_text['num_unique_words'] / df_text['num_words']  \n",
    "    return df_text\n",
    "\n",
    "train = add_features(df_text)\n",
    "features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "ss = StandardScaler()\n",
    "ss.fit(features)\n",
    "features = ss.transform(features)\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(df_text['text']))\n",
    "train_X = tokenizer.texts_to_sequences(df_text['text'])\n",
    "\n",
    "## pad sequences\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "\n",
    "#word index\n",
    "word_index=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Distribution graphs (histogram/bar graph) of column data\n",
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f'{columnNames[i]} (column {i})')\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "def plotCorrelationMatrix(df, graphWidth):\n",
    "    filename = df.dataframeName\n",
    "    df = df.dropna('columns') # drop columns with NaN\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30697 rows and 28 columns\n"
     ]
    }
   ],
   "source": [
    "nRowsRead = None\n",
    "df1 = pd.read_csv('data.csv', delimiter=',', nrows = nRowsRead)\n",
    "df1.dataframeName = 'data.csv'\n",
    "nRow, nCol = df1.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution graphs (histogram/bar graph) of sampled columns:\n",
    "#plotPerColumnDistribution(df1, 10, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix:\n",
    "#plotCorrelationMatrix(df1, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = 'glove.6B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE,encoding=\"utf8\"))\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    # Why random embedding for OOV? what if use mean?\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size)) # std 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = 'wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE,encoding=\"utf8\") if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "glove_embeddings = load_glove(word_index)\n",
    "#paragram_embeddings = load_para(word_index)\n",
    "fasttext_embeddings = load_fasttext(word_index)\n",
    "embedding_matrix = np.mean([glove_embeddings,fasttext_embeddings], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int32, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int32, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "text=pd.DataFrame(train_X)\n",
    "df=df.join(text)\n",
    "df_test=df.loc[df.is_goal.isnull()]\n",
    "df=df.drop(df[df.is_goal.isnull()].index) \n",
    "df=df.drop(\"shot_id_number\", axis=1)\n",
    "df_goal1=df['is_goal']\n",
    "df=df.drop(\"is_goal\", axis=1)\n",
    "df=df.join(df_goal1)\n",
    "df = df.fillna(df.mean())\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(df.iloc[:,:]) \n",
    "df.iloc[:,:] = scaled_values\n",
    "df_test=df_test.drop('is_goal',axis=1)\n",
    "df_test_shotid=df_test['shot_id_number']\n",
    "df_test=df_test.drop('shot_id_number',axis=1)\n",
    "df_test = df_test.fillna(df.mean())\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(df_test.iloc[:,:]) \n",
    "df_test.iloc[:,:] = scaled_values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.fillna(df.mean())\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(df_test.iloc[:,:]) \n",
    "df_test.iloc[:,:] = scaled_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df.iloc[:,0:-1 ])\n",
    "y = df.iloc[:, -1].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.25, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def train_model(classifier, X_train, y_train, X_test, y_test,is_neural_net=False):\n",
    "    classifier.fit(X_train, y_train,epochs=5)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import activations\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "\n",
    "#define our own softmax function instead of K.softmax\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex/K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "#A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'squash':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        #final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:,:,:,0]) #shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            c = softmax(b, 1)\n",
    "            o = K.batch_dot(c, u_hat_vecs, [2, 2])\n",
    "            if K.backend() == 'theano':\n",
    "                o = K.sum(o, axis=1)\n",
    "            if i < self.routings - 1:\n",
    "                o = K.l2_normalize(o, -1)\n",
    "                b = K.batch_dot(o, u_hat_vecs, [2, 3])\n",
    "                if K.backend() == 'theano':\n",
    "                    b = K.sum(b, axis=1)\n",
    "\n",
    "        return self.activation(o)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 113, 300)          15000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 113, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 113, 512)          855552    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 57856)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 57857     \n",
      "=================================================================\n",
      "Total params: 928,409\n",
      "Trainable params: 928,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 18321 samples, validate on 6108 samples\n",
      "Epoch 1/1\n",
      "18321/18321 [==============================] - 448s 24ms/step - loss: 8.8792 - acc: 0.4430 - val_loss: 8.7333 - val_acc: 0.4522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e322f51518>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "\n",
    "#from vendor.Capsule.Capsule_Keras import *\n",
    "\n",
    "gru_len = 256\n",
    "Routings = 3\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "embed_size = 300\n",
    "\n",
    "def get_model():\n",
    "    input1 = Input(shape=(113,))\n",
    "    embed_layer = Embedding(max_features,\n",
    "                            embed_size,\n",
    "                            input_length=113)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(GRU(gru_len,\n",
    "                          activation='relu',\n",
    "                          dropout=dropout_p,\n",
    "                          recurrent_dropout=dropout_p,\n",
    "                          return_sequences=True))(embed_layer)\n",
    "    capsule = Capsule(\n",
    "        num_capsule=Num_capsule,\n",
    "        dim_capsule=Dim_capsule,\n",
    "        routings=Routings,\n",
    "        share_weights=True)(x)\n",
    "\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    capsule = LeakyReLU()(capsule)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    output = Dense(1, activation='softmax')(x)\n",
    "    model1 = Model(inputs=input1, outputs=output)\n",
    "    model1.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    model1.summary()\n",
    "\n",
    "    return model1\n",
    "\n",
    "\n",
    "model1 = get_model()\n",
    "batch_size = 450\n",
    "epochs = 5\n",
    "\n",
    "model1.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.predict(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4957044901961818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae=mean_absolute_error(y_valid,predictions)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_predict=model.predict_proba(df_test)[:,1]\n",
    "prob= pd.DataFrame(goal_predict)\n",
    "prob['is_goal'] = prob[0]\n",
    "prob = prob.drop([0], axis = 1)\n",
    "df_test_shotid = df_test_shotid.fillna(df_test_shotid.mean())\n",
    "df_test_shotid=pd.DataFrame(df_test_shotid)\n",
    "df_test_shotid=df_test_shotid.join(prob['is_goal'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_shotid.to_csv(r'C:\\Ghost Drive\\study\\research\\zs/Abhishek_Kathuria_20041998_code_3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
